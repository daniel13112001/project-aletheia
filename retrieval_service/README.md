# Retrieval Service

A gRPC server that performs vector similarity search for fact-checking queries. Converts text queries into embeddings using OpenAI and searches a FAISS index for similar claims.

## Tech Stack

- **Language:** Python 3.11
- **Framework:** gRPC
- **Vector Store:** FAISS
- **Embeddings:** OpenAI (`text-embedding-3-small`)

## Prerequisites

- Python 3.11+
- OpenAI API key
- Pre-built FAISS index (generated by the Ingestion Service)

## Installation

### Local Development

```bash
cd retrieval_service

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Docker

```bash
# From project root
docker build -t aletheia-retrieval -f retrieval_service/Dockerfile .
```

## Configuration

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes | OpenAI API key for generating embeddings |

Create a `.env` file or export the variable:
```bash
export OPENAI_API_KEY=sk-...
```

### FAISS Index Path

The FAISS index path is currently hardcoded in `server.py`:
```python
vector_store = FaissVectorStore.load("/path/to/artifacts/faiss")
```

Update this path to match your environment, or mount the artifacts directory when using Docker.

## Running

### Local Development

```bash
cd retrieval_service
python -m retrieval_service.server
```

The server starts on port **50051**.

### Docker

```bash
docker run -p 50051:50051 \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  -v /path/to/artifacts:/app/artifacts \
  aletheia-retrieval
```

## gRPC API

### Service Definition

```protobuf
service VectorSearchService {
  rpc Search (SearchRequest) returns (SearchResponse);
}
```

### Search Method

Performs vector similarity search for a given query.

**Request:**
```protobuf
message SearchRequest {
  string query = 1;  // The text to search for
  uint32 k = 2;      // Number of results to return (default: 3)
}
```

**Response:**
```protobuf
message SearchResponse {
  repeated SearchResult results = 1;
}

message SearchResult {
  string uid = 1;    // Unique identifier of the matched claim
  float score = 2;   // Similarity score (higher = more similar)
}
```

### Example Usage (Python)

```python
import grpc
from retrieval_service import vector_search_pb2, vector_search_pb2_grpc

# Connect to the service
channel = grpc.insecure_channel('localhost:50051')
stub = vector_search_pb2_grpc.VectorSearchServiceStub(channel)

# Make a search request
request = vector_search_pb2.SearchRequest(
    query="The earth is flat",
    k=3
)
response = stub.Search(request)

# Process results
for result in response.results:
    print(f"UID: {result.uid}, Score: {result.score}")
```

### Example Usage (Go)

```go
conn, _ := grpc.Dial("localhost:50051", grpc.WithInsecure())
client := vectorsearch.NewVectorSearchServiceClient(conn)

resp, _ := client.Search(ctx, &vectorsearch.SearchRequest{
    Query: "The earth is flat",
    K:     3,
})

for _, r := range resp.Results {
    fmt.Printf("UID: %s, Score: %f\n", r.Uid, r.Score)
}
```

## Architecture

```
┌────────────────────────────────────────────────────────┐
│                   Retrieval Service                     │
│                                                         │
│  ┌─────────────────────────────────────────────────┐   │
│  │              VectorSearchServicer                │   │
│  │                                                  │   │
│  │  1. Receive query text                          │   │
│  │  2. Generate embedding via OpenAI               │   │
│  │  3. Search FAISS index                          │   │
│  │  4. Return UIDs and scores                      │   │
│  └──────────────────┬──────────────────────────────┘   │
│                     │                                   │
│         ┌───────────┴───────────┐                      │
│         ▼                       ▼                      │
│  ┌─────────────┐       ┌─────────────────┐            │
│  │   OpenAI    │       │   FAISS Index   │            │
│  │   API       │       │   (Vector DB)   │            │
│  └─────────────┘       └─────────────────┘            │
└────────────────────────────────────────────────────────┘
```

## Project Structure

```
retrieval_service/
├── server.py              # gRPC server implementation
├── vector_search_pb2.py   # Generated protobuf messages
├── vector_search_pb2_grpc.py  # Generated gRPC stubs
├── requirements.txt       # Python dependencies
├── Dockerfile             # Container definition
└── __init__.py
```

## Dependencies

Key packages from `requirements.txt`:

| Package | Version | Purpose |
|---------|---------|---------|
| grpcio | 1.76.0 | gRPC framework |
| openai | 2.14.0 | OpenAI API client |
| faiss-cpu | 1.13.2 | Vector similarity search |
| protobuf | 6.33.2 | Protocol buffers |
| python-dotenv | - | Environment variable loading |

## Vector Search Details

### Embedding Model

- **Model:** `text-embedding-3-small`
- **Dimensions:** 1536
- **Provider:** OpenAI

### FAISS Index

- **Index Type:** `IndexFlatIP` (Inner Product)
- **Normalization:** L2 normalization applied to vectors
- **Similarity:** Cosine similarity (via normalized inner product)

### Search Process

1. Query text is sent to OpenAI API
2. Embedding vector (1536 dimensions) is returned
3. Vector is L2-normalized
4. FAISS index is searched for k nearest neighbors
5. UIDs and similarity scores are returned

## Graceful Shutdown

The server handles `SIGTERM` and `SIGINT` signals for graceful shutdown:

```python
signal.signal(signal.SIGTERM, lambda *_: server.stop(0))
signal.signal(signal.SIGINT, lambda *_: server.stop(0))
```

## Regenerating gRPC Code

If the proto file changes, regenerate the Python code:

```bash
python -m grpc_tools.protoc \
  -I../proto \
  --python_out=. \
  --grpc_python_out=. \
  ../proto/vector_search.proto
```

## Troubleshooting

**"OPENAI_API_KEY is not set":**
- Ensure the environment variable is set before starting the server
- Check that `.env` file exists if using python-dotenv

**FAISS index not found:**
- Verify the index path in `server.py`
- Run the Ingestion Service first to generate the index
- Check that `index.faiss` and `ids.json` exist in the artifacts folder

**gRPC connection refused:**
- Ensure the server is running on port 50051
- Check firewall settings if running in Docker
